{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Install Required Libraries","metadata":{"_uuid":"6e711478-8fa0-41d2-8311-f9ed7fcc5263","_cell_guid":"378abfdf-0384-430c-8a08-87d088d136b8","trusted":true}},{"cell_type":"code","source":"!pip install transformers datasets wandb -q","metadata":{"_uuid":"5c2ff626-bcf0-4ff9-8816-22dcc461bd93","_cell_guid":"4a39d4fa-63a9-4c81-8f06-cfd70d4ad7c9","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import Required Libraries","metadata":{"_uuid":"6e06770e-fbe9-462e-8240-6a817e11ef1c","_cell_guid":"3aa1f815-1322-4951-8903-e6269e8dd686","trusted":true}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer, TrainingArguments, Trainer\nfrom datasets import load_dataset\nimport wandb\nimport os","metadata":{"_uuid":"954fcc98-7b8a-4b39-ac49-a748fc6406f3","_cell_guid":"3dcfdae6-0ee6-4a61-9884-9630ec1c4407","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define Hyperparameters","metadata":{"_uuid":"15132731-dd9d-4cba-801b-8c74b342e979","_cell_guid":"efb13f32-e55b-4f3b-8c05-d043a896aa9a","trusted":true}},{"cell_type":"code","source":"model_name = \"GanjinZero/biobart-v2-base\"\nmax_seq_length = 512\nlearning_rate = 2e-5\nweight_decay = 0.01\nmax_steps = 500\nwarmup_steps = 100\nbatch_size = 4\ngradient_accumulation_steps = 4\nlr_scheduler_type = \"linear\"\noptimizer = \"adamw_hf\"\nrandom_state = 3407\noutput_dir = \"./biobart-finetuned\"","metadata":{"_uuid":"bd45afda-1767-4af2-a05a-5f7b2e1dd8f9","_cell_guid":"dfdd3aae-0994-4e50-bd56-1ae34cab7f3f","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load BioBart Model and Tokenizer","metadata":{"_uuid":"a6ab9ecb-46c4-4dbd-a907-4fd1e8f2c14e","_cell_guid":"dcb3d0e8-859c-4742-9f22-e72ba7114388","trusted":true}},{"cell_type":"code","source":"model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"_uuid":"94f6cb11-4bfb-4d9e-9d6c-2994a1afae71","_cell_guid":"72232ce4-917e-4cc5-b87b-abec29f355f6","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import inspect\nprint(inspect.signature(model.forward))","metadata":{"execution":{"iopub.status.busy":"2024-10-07T04:20:42.340031Z","iopub.execute_input":"2024-10-07T04:20:42.340442Z","iopub.status.idle":"2024-10-07T04:20:42.349791Z","shell.execute_reply.started":"2024-10-07T04:20:42.340397Z","shell.execute_reply":"2024-10-07T04:20:42.348722Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"(input_ids: torch.LongTensor = None, attention_mask: Optional[torch.Tensor] = None, decoder_input_ids: Optional[torch.LongTensor] = None, decoder_attention_mask: Optional[torch.LongTensor] = None, head_mask: Optional[torch.Tensor] = None, decoder_head_mask: Optional[torch.Tensor] = None, cross_attn_head_mask: Optional[torch.Tensor] = None, encoder_outputs: Optional[List[torch.FloatTensor]] = None, past_key_values: Optional[List[torch.FloatTensor]] = None, inputs_embeds: Optional[torch.FloatTensor] = None, decoder_inputs_embeds: Optional[torch.FloatTensor] = None, labels: Optional[torch.LongTensor] = None, use_cache: Optional[bool] = None, output_attentions: Optional[bool] = None, output_hidden_states: Optional[bool] = None, return_dict: Optional[bool] = None) -> Union[Tuple, transformers.modeling_outputs.Seq2SeqLMOutput]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Load Dataset (Here we are using Wiki Medical Terms as an example)","metadata":{"_uuid":"4c3dbe5c-3328-4b9f-a936-c75b680c7b68","_cell_guid":"11c451b0-f804-4a9d-bfed-8e53edc33e5d","trusted":true}},{"cell_type":"code","source":"# Load dataset from Hugging Face Hub\ndataset = load_dataset(\"gamino/wiki_medical_terms\", split=\"train\")","metadata":{"_uuid":"22cf8701-bde3-4587-99ec-9aa43105972b","_cell_guid":"416c076d-dc69-4a8b-a87b-726e45dc75f5","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing for Seq2Seq Models","metadata":{"_uuid":"4e72dcf5-e2ae-404c-9800-6dd9f13a806b","_cell_guid":"1aad055b-3e21-4a8d-9498-7af6159a94ee","trusted":true}},{"cell_type":"code","source":"def preprocess_function(examples):\n    max_length = 128  # Set your preferred maximum length here\n    \n    # Tokenize the page_text as the input (prompt)\n    inputs = tokenizer(examples['page_text'], padding=\"max_length\", truncation=True, max_length=max_length)\n\n    # Tokenize the page_title as the labels (for supervised learning tasks)\n    labels = tokenizer(examples['page_title'], padding=\"max_length\", truncation=True, max_length=max_length)\n\n    # Return the required fields for the model's forward method\n    return {\n        'input_ids': inputs['input_ids'],\n        'attention_mask': inputs['attention_mask'],\n        'labels': labels['input_ids']  # Labels should be the tokenized page_title\n    }\n\n# Apply preprocessing to the dataset\ntokenized_dataset = dataset.map(preprocess_function, batched=True)\n","metadata":{"_uuid":"6229cbb3-5ba6-4a57-8db1-3d604a93c501","_cell_guid":"eb5e1d4f-ba8a-45e4-b118-75263d48437e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-07T04:25:44.140615Z","iopub.execute_input":"2024-10-07T04:25:44.141067Z","iopub.status.idle":"2024-10-07T04:26:01.549243Z","shell.execute_reply.started":"2024-10-07T04:25:44.141027Z","shell.execute_reply":"2024-10-07T04:26:01.548420Z"},"trusted":true},"execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6861 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1d1500b6ef840d6a4182ae65750ad48"}},"metadata":{}}]},{"cell_type":"code","source":"# Print the first few examples of tokenized data\nfor i in range(1):  # Change the range to print more examples if needed\n    print(tokenized_dataset[0])\n","metadata":{"execution":{"iopub.status.busy":"2024-10-07T04:24:53.340546Z","iopub.execute_input":"2024-10-07T04:24:53.341486Z","iopub.status.idle":"2024-10-07T04:24:53.354991Z","shell.execute_reply.started":"2024-10-07T04:24:53.341442Z","shell.execute_reply":"2024-10-07T04:24:53.353917Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"{'page_title': 'Paracetamol poisoning', 'page_text': 'Paracetamol poisoning, also known as acetaminophen poisoning, is caused by excessive use of the medication paracetamol (acetaminophen). Most people have few or non-specific symptoms in the first 24 hours following overdose. These include feeling tired, abdominal pain, or nausea. This is typically followed by a couple of days without any symptoms, after which yellowish skin, blood clotting problems, and confusion occurs as a result of liver failure. Additional complications may include kidney failure, pancreatitis, low blood sugar, and lactic acidosis. If death does not occur, people tend to recover fully over a couple of weeks. Without treatment, death from toxicity occurs 4 to 18 days later.Paracetamol poisoning can occur accidentally or as an attempt to die by suicide. Risk factors for toxicity include alcoholism, malnutrition, and the taking of certain other hepatotoxic medications. Liver damage results not from paracetamol itself, but from one of its metabolites, N-acetyl-p-benzoquinone imine (NAPQI). NAPQI decreases the livers glutathione and directly damages cells in the liver. Diagnosis is based on the blood level of paracetamol at specific times after the medication was taken. These values are often plotted on the Rumack-Matthew nomogram to determine level of concern.Treatment may include activated charcoal if the person seeks medical help soon after the overdose. Attempting to force the person to vomit is not recommended. If there is a potential for toxicity, the antidote acetylcysteine is recommended. The medication is generally given for at least 24 hours. Psychiatric care may be required following recovery. A liver transplant may be required if damage to the liver becomes severe. The need for transplant is often based on low blood pH, high blood lactate, poor blood clotting, or significant hepatic encephalopathy. With early treatment liver failure is rare. Death occurs in about 0.1% of cases.Paracetamol poisoning was first described in the 1960s. Rates of poisoning vary significantly between regions of the world. In the United States more than 100,000 cases occur a year. In the United Kingdom it is the medication responsible for the greatest number of overdoses. Young children are most commonly affected. In the United States and the United Kingdom, paracetamol is the most common cause of acute liver failure.\\n\\nSigns and symptoms\\nThe signs and symptoms of paracetamol toxicity occur in three phases. The first phase begins within hours of overdose, and consists of nausea, vomiting, a pale appearance, and sweating. However, patients often have no specific symptoms or only mild symptoms in the first 24 hours of poisoning. Rarely, after massive overdoses, patients may develop symptoms of metabolic acidosis and coma early in the course of poisoning.The second phase occurs between 24 hours and 72 hours following overdose and consists of signs of increasing liver damage. In general, damage occurs in liver cells as they metabolize the paracetamol. The individual may experience right upper quadrant abdominal pain. The increasing liver damage also changes biochemical markers of liver function; International normalized ratio (INR) and the liver transaminases ALT and AST rise to abnormal levels. Acute kidney failure may also occur during this phase, typically caused by either hepatorenal syndrome or multiple organ dysfunction syndrome. In some cases, acute kidney failure may be the primary clinical manifestation of toxicity. In these cases, it has been suggested that the toxic metabolite is produced more in the kidneys than in the liver.The third phase follows at 3 to 5 days, and is marked by complications of massive liver necrosis leading to fulminant liver failure with complications of coagulation defects, low blood sugar, kidney failure, hepatic encephalopathy, brain swelling, sepsis, multiple organ failure, and death. If the third phase is survived, the liver necrosis runs its course, and liver and kidney function typically return to normal in a few weeks. The severity of paracetamol toxicity varies depending on the dose and whether appropriate treatment is received.\\n\\nCause\\nThe toxic dose of paracetamol is highly variable. In general the recommended maximum daily dose for healthy adults is 4 grams. Higher doses lead to increasing risk of toxicity. In adults, single doses above 10 grams or 200 mg/kg of bodyweight, whichever is lower, have a reasonable likelihood of causing toxicity. Toxicity can also occur when multiple smaller doses within 24 hours exceed these levels. Following a dose of 1 gram of paracetamol four times a day for two weeks, patients can expect an increase in alanine transaminase in their liver to typically about three times the normal value. It is unlikely that this dose would lead to liver failure. Studies have shown significant hepatotoxicity is uncommon in patients who have taken greater than normal doses over 3 to 4 days. In adults, a dose of 6 grams a day over the preceding 48 hours could potentially lead to toxicity, while in children acute doses above 200 mg/kg could potentially cause toxicity. Acute paracetamol overdose in children rarely causes illness or death, and it is very uncommon for children to have levels that require treatment, with chronic larger-than-normal doses being the major cause of toxicity in children.Intentional overdosing (self-poisoning, with suicidal intent) is frequently implicated in paracetamol toxicity. In a 2006 review, paracetamol was the most frequently ingested compound in intentional overdosing.In rare individuals, paracetamol toxicity can result from normal use. This may be due to individual (\"idiosyncratic\") differences in the expression and activity of certain enzymes in one of the metabolic pathways that handle paracetamol (see paracetamols metabolism).\\n\\nRisk factors\\nA number of factors can potentially increase the risk of developing paracetamol toxicity. Chronic excessive alcohol consumption can induce CYP2E1, thus increasing the potential toxicity of paracetamol. In one study of patients with liver injury, 64% reported alcohol intakes of greater than 80 grams a day, while 35% took 60 grams a day or less. Whether chronic alcoholism should be considered a risk factor has been debated by some clinical toxicologists. For chronic alcohol users, acute alcohol ingestion at the time of a paracetamol overdose may have a protective effect. For non-chronic alcohol users, acute alcohol consumption had no protective effect.\\nFasting is a risk factor, possibly because of depletion of liver glutathione reserves. The concomitant use of the CYP2E1 inducer isoniazid increases the risk of hepatotoxicity, though whether 2E1 induction is related to the hepatotoxicity in this case is unclear. Concomitant use of other drugs that induce CYP enzymes, such as antiepileptics including carbamazepine, phenytoin, and barbiturates, have also been reported as risk factors.\\n\\nPathophysiology\\nWhen taken in normal therapeutic doses, paracetamol has been shown to be safe. Following a therapeutic dose, it is mostly converted to nontoxic metabolites via Phase II metabolism by conjugation with sulfate and glucuronide, with a small portion being oxidized via the cytochrome P450 enzyme system. Cytochromes P450 2E1 and 3A4 convert approximately 5% of paracetamol to a highly reactive intermediary metabolite, N-acetyl-p-benzoquinone imine (NAPQI). Under normal conditions, NAPQI is detoxified by conjugation with glutathione to form cysteine and mercapturic acid conjugates.In cases of paracetamol overdose, the sulfate and glucuronide pathways become saturated, and more paracetamol is shunted to the cytochrome P450 system to produce NAPQI. As a result, hepatocellular supplies of glutathione become depleted, as the demand for glutathione is higher than its regeneration. NAPQI therefore remains in its toxic form in the liver and reacts with cellular membrane molecules, resulting in widespread hepatocyte damage and death, leading to acute liver necrosis. In animal studies, the livers stores of glutathione must be depleted to less than 70% of normal levels before liver toxicity occurs.\\n\\nDiagnosis\\nA persons history of taking paracetamol is somewhat accurate for the diagnosis. The most effective way to diagnose poisoning is by obtaining a blood paracetamol level. A drug nomogram developed in 1975, called the Rumack-Matthew nomogram, estimates the risk of toxicity based on the serum concentration of paracetamol at a given number of hours after ingestion. To determine the risk of potential hepatotoxicity, the paracetamol level is traced along the nomogram. Use of a timed serum paracetamol level plotted on the nomogram appears to be the best marker indicating the potential for liver injury. A paracetamol level drawn in the first four hours after ingestion may underestimate the amount in the system because paracetamol may still be in the process of being absorbed from the gastrointestinal tract. Therefore, a serum level taken before 4 hours is not recommended.Clinical or biochemical evidence of liver toxicity may develop in one to four days, although, in severe cases, it may be evident in 12 hours. Right-upper-quadrant tenderness may be present and can aid in diagnosis. Laboratory studies may show evidence of liver necrosis with elevated AST, ALT, bilirubin, and prolonged coagulation times, particularly an elevated prothrombin time. After paracetamol overdose, when AST and ALT exceed 1000 IU/L, paracetamol-induced hepatotoxicity can be diagnosed. In some cases, the AST and ALT levels can exceed 10,000 IU/L.\\n\\nDetection in body fluids\\nParacetamol may be quantified in blood, plasma, or urine as a diagnostic tool in clinical poisoning situations or to aid in the medicolegal investigation of suspicious deaths. The concentration in serum after a typical dose of paracetamol usually peaks below 30 mg/L, which equals 200 μmol/L. Levels of 30–300 mg/L (200–2000 μmol/L) are often observed in overdose patients. Postmortem blood levels have ranged from 50 to 400 mg/L in persons dying due to acute overdosage. Automated colorimetric techniques, gas chromatography and liquid chromatography are currently in use for the laboratory analysis of the drug in physiological specimens.\\n\\nPrevention\\nLimitation of availability\\nLimiting the availability of paracetamol tablets has been attempted in some countries. In the UK, sales of over-the-counter paracetamol are restricted to packs of 32 x 500 mg tablets in pharmacies, and 16 x 500 mg tablets in non-pharmacy outlets. Pharmacists may provide up to 100 tablets for those with chronic conditions at the pharmacists discretion. In Ireland, the limits are 24 and 12 tablets, respectively. Subsequent study suggests that the reduced availability in large numbers had a significant effect in reducing poisoning deaths from paracetamol overdose.One suggested method of prevention is to make paracetamol a prescription-only medicine, or to remove it entirely from the market. However, overdose is a relatively minor problem; for example, 0.08% of the UK population (over 50 thousand people) present with paracetamol overdose each year. In contrast, paracetamol is a safe and effective medication that is taken without complications by millions of people. In addition, alternative pain relief medications such as aspirin are more toxic in overdose, whereas non-steroidal anti-inflammatory drugs are associated with more adverse effects following normal use.\\n\\nCombination with other agents\\nOne strategy for reducing harm done by acetaminophen overdoses is selling paracetamol pre-combined in tablets either with an emetic or an antidote. Paradote was a tablet sold in the UK which combined 500 mg paracetamol with 100 mg methionine, an amino acid formerly used in the treatment of paracetamol overdose.\\nThere have been no studies so far on the effectiveness of paracetamol when given in combination with its most commonly used antidote, acetylcysteine.Calcitriol, the active metabolite of vitamin D3, appears to be a catalyst for glutathione production. Calcitriol was found to increase glutathione levels in rat astrocyte primary cultures on average by 42%, increasing glutathione protein concentrations from 29 nmol/mg to 41 nmol/mg, 24 and 48 hours after administration; it continued to have an influence on glutathione levels 96 hours after administration. It has been proposed that co-administration of calcitriol, via injection, may improve treatment outcomes.\\n\\nParacetamol replacements\\nParacetamol ester prodrug with L-pyroglutamic acid (PCA), a biosynthetic precursor of glutathione, has been synthesized to reduce paracetamol hepatotoxicity and improve bioavailability. The toxicological studies of different paracetamol esters show that L-5-oxo-pyrrolidine-2-paracetamol carboxylate reduces toxicity after administration of an overdose of paracetamol to mice. The liver glutathione values in mice induced by intraperitoneal injection of the ester are superimposable with the GSH levels recorded in untreated mice control group. The mice group treated with an equivalent dose of paracetamol showed a significative decrease of glutathione of 35% (p<0.01 vs untreated control group). The oral LD50 was found to be greater than 2000 mg kg-1, whereas the intraperitoneal LD50 was 1900 mg kg-1. These results taken together with the good hydrolysis and bioavailability data show that this ester is a potential candidate as a prodrug of paracetamol.\\n\\nTreatment\\nGastric decontamination\\nIn adults, the initial treatment for paracetamol overdose is gastrointestinal decontamination. Paracetamol absorption from the gastrointestinal tract is complete within two hours under normal circumstances, so decontamination is most helpful if performed within this timeframe. Gastric lavage, better known as stomach pumping, may be considered if the amount ingested is potentially life-threatening and the procedure can be performed within 60 minutes of ingestion. Activated charcoal is the most common gastrointestinal decontamination procedure as it adsorbs paracetamol, reducing its gastrointestinal absorption. Administering activated charcoal also poses less risk of aspiration than gastric lavage.It appears that the most benefit from activated charcoal is gained if it is given within 30 minutes to two hours of ingestion. Administering activated charcoal later than 2 hours can be considered in patients that may have delayed gastric emptying due to co-ingested drugs or following ingestion of sustained- or delayed-release paracetamol preparations. Activated charcoal should also be administered if co-ingested drugs warrant decontamination. There was reluctance to give activated charcoal in paracetamol overdose, because of the concern that it may also absorb the oral antidote acetylcysteine. Studies have shown that 39% less acetylcysteine is absorbed into the body when they are administered together. There are conflicting recommendations regarding whether to change the dosing of oral acetylcysteine after the administration of activated charcoal, and even whether the dosing of acetylcysteine needs to be altered at all. Intravenous acetylcysteine has no interaction with activated charcoal.\\nInducing vomiting with syrup of ipecac has no role in paracetamol overdose because the vomiting it induces delays the effective administration of activated charcoal and oral acetylcysteine. Liver injury is extremely rare after acute accidental ingestion in children under 6 years of age. Children with accidental exposures do not require gastrointestinal decontamination with either gastric lavage, activated charcoal, or syrup of ipecac.\\n\\nAcetylcysteine\\nAcetylcysteine, also called N-acetylcysteine or NAC, works to reduce paracetamol toxicity by replenishing body stores of the antioxidant glutathione. Glutathione reacts with the toxic NAPQI metabolite so that it does not damage cells and can be safely excreted. NAC was usually given following a treatment nomogram (one for patients with risk factors, and one for those without) but the use of the nomogram is no longer recommended as the evidence base to support the use of risk factors was poor and inconsistent and many of the risk factors are imprecise and difficult to determine with sufficient certainty in clinical practice. Cysteamine and methionine have also been used to prevent hepatotoxicity, although studies show that both are associated with more adverse effects than acetylcysteine. Additionally, acetylcysteine has been shown to be a more effective antidote, particularly in patients presenting greater than 8 hours post-ingestion and for those who present with liver failure symptoms.If the person presents less than eight hours after paracetamol overdose, then acetylcysteine significantly reduces the risk of serious hepatotoxicity and guarantees survival. If acetylcysteine is started more than 8 hours after ingestion, there is a sharp decline in its effectiveness because the cascade of toxic events in the liver has already begun, and the risk of acute liver necrosis and death increases dramatically. Although acetylcysteine is most effective if given early, it still has beneficial effects if given as late as 48 hours after ingestion. If the person presents more than eight hours after the paracetamol overdose, then activated charcoal is not useful, and acetylcysteine is started immediately. In earlier presentations, charcoal can be given when the patient arrives and acetylcysteine is initiated while waiting for the paracetamol level results to return from the laboratory.In United States practice, intravenous (IV) and oral administration are considered to be equally effective and safe if given within 8 hours of ingestion. However, IV is the only recommended route in Australasian and British practice. Oral acetylcysteine is given as a 140 mg/kg loading dose followed by 70 mg/kg every four hours for 17 more doses, and if the patient vomits within 1 hour of dose, the dose must be repeated. Oral acetylcysteine may be poorly tolerated due to its unpleasant taste, odor, and its tendency to cause nausea and vomiting. If repeated doses of charcoal are indicated because of another ingested drug, then subsequent doses of charcoal and acetylcysteine should be staggered.Intravenous acetylcysteine is given as a continuous infusion over 20 hours for a total dose 300 mg/kg. Recommended administration involves infusion of a 150 mg/kg loading dose over 15 to 60 minutes, followed by a 50 mg/kg infusion over four hours; the last 100 mg/kg are infused over the remaining 16 hours of the protocol. Intravenous acetylcysteine has the advantage of shortening hospital stay, increasing both doctor and patient convenience, and allowing administration of activated charcoal to reduce absorption of both the paracetamol and any co-ingested drugs without concerns about interference with oral acetylcysteine.  Intravenous dosing varies with weight, specifically in children. For patients less than 20 kg, the loading dose is 150 mg/kg in 3 mL/kg diluent, administered over 60 minutes; the second dose is 50 mg/kg in 7 mL/kg diluent over 4 hours; and the third and final dose is 100 mg/kg in 14 mL/kg diluent over 16 hours.The most common adverse effect to acetylcysteine treatment is an anaphylactoid reaction, usually manifested by rash, wheeze, or mild hypotension. May cause infertility or death. Adverse reactions are more common in people treated with IV acetylcysteine, occurring in up to 20% of patients. Anaphylactoid reactions are more likely to occur with the first infusion (the loading dose). Rarely, severe life-threatening reactions may occur in predisposed individuals, such as patients with asthma or atopic dermatitis, and may be characterized by respiratory distress, facial swelling, and even death.If an anaphylactoid reaction occurs the acetylcysteine is temporarily halted or slowed and antihistamines and other supportive care is administered. For example, a nebulised beta-agonist like salbutamol may be indicated in the event of significant bronchospasm (or prophylactically in patients with a history of bronchospasm secondary to acetylcysteine). It is also important to closely monitor fluids and electrolytes.\\n\\nLiver transplant\\nIn people who develop acute liver failure or who are otherwise expected to die from liver failure, the mainstay of management is liver transplantation. Liver transplants are performed in specialist centers. The most commonly used criteria for liver transplant were developed by physicians at Kings College Hospital in London. Patients are recommended for transplant if they have an arterial blood pH less than 7.3 after fluid resuscitation or if a patient has Grade III or IV encephalopathy, a prothrombin time greater than 100 seconds, and a serum creatinine greater than 300 mmol/L In a 24-hour period. Other forms of liver support have been used including partial liver transplants. These techniques have the advantage of supporting the patient while their own liver regenerates. Once liver function returns immunosuppressive drugs are commenced and they have to take immunosuppressive medication for the rest of their lives.\\n\\nPrognosis\\nThe mortality rate from paracetamol overdose increases two days after the ingestion, reaches a maximum on day four, and then gradually decreases. Acidosis is the most important single indicator of probable mortality and the need for transplantation. A mortality rate of 95% without transplant was reported in patients who had a documented pH less than 7.30. Other indicators of poor prognosis include chronic kidney disease (stage 3 or worse), hepatic encephalopathy, a markedly elevated prothrombin time, or an elevated blood lactic acid level (lactic acidosis). One study has shown that a factor V level less than 10% of normal indicated a poor prognosis (91% mortality), whereas a ratio of factor VIII to factor V of less than 30 indicated a good prognosis (100% survival). Patients with a poor prognosis are usually identified for likely liver transplantation. Patients that do not die are expected to fully recover and have a normal life expectancy and quality of life.\\n\\nEpidemiology\\nMany over-the-counter and prescription-only medications contain paracetamol. Because of its wide availability paired with comparably high toxicity, (compared to ibuprofen and aspirin) there is a much higher potential for overdose. Paracetamol toxicity is one of the most common causes of poisoning worldwide. In the United States, the United Kingdom, Australia, and New Zealand, paracetamol is the most common cause of drug overdoses. Additionally, in both the United States and the United Kingdom it is the most common cause of acute liver failure.In England and Wales an estimated 41,200 cases of paracetamol poisoning occurred in 1989 to 1990, with a mortality of 0.40%. It is estimated that 150 to 200 deaths and 15 to 20 liver transplants occur as a result of poisoning each year in England and Wales. Paracetamol overdose results in more calls to poison control centers in the US than overdose of any other pharmacological substance, accounting for more than 100,000 calls, as well as 56,000 emergency room visits, 2,600 hospitalizations, and 458 deaths due to acute liver failure per year. A study of cases of acute liver failure between November 2000 and October 2004 by the Centers for Disease Control and Prevention in the USA found that paracetamol was the cause of 41% of all cases in adults, and 25% of cases in children.\\n\\nReferences\\nExternal links\\nGerth, Jeff; T. Christian Miller (September 20, 2013). \"Use Only as Directed\". ProPublica. Retrieved October 12, 2013.', '__index_level_0__': 0, 'input_ids': [0, 22011, 1043, 67369, 1168, 15000, 6, 67, 684, 25, 37662, 77748, 2457, 15000, 6, 16, 1726, 30, 10079, 304, 9, 1437, 627, 8456, 53813, 67369, 1168, 36, 26799, 77748, 2457, 322, 1993, 82, 33, 367, 50, 786, 12, 14175, 5298, 11, 1437, 627, 78, 706, 722, 511, 11972, 4, 1216, 680, 2157, 1437, 90, 7651, 6, 28670, 2400, 6, 50, 27214, 4, 152, 16, 1437, 44661, 1432, 30, 10, 891, 9, 360, 396, 143, 5298, 6, 71, 61, 81411, 3024, 6, 1925, 31293, 2577, 1272, 6, 8, 9655, 11493, 25, 10, 898, 9, 12581, 2988, 4, 7655, 3137, 52613, 189, 680, 12855, 2988, 6, 30737, 10100, 6, 614, 1925, 4696, 6, 8, 54801, 54632, 4, 318, 744, 473, 45, 5948, 6, 82, 1437, 90, 1397, 1437, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [0, 22011, 1043, 67369, 1168, 15000, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Define Training Arguments","metadata":{"_uuid":"f6aa3d3e-6cf4-4e78-a4b1-b7552b2b7f8a","_cell_guid":"9fbfffd3-aa65-49ba-ab71-255d6163a3f0","trusted":true}},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=output_dir,\n    per_device_train_batch_size=batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    warmup_steps=warmup_steps,\n    max_steps=max_steps,\n    learning_rate=learning_rate,\n    weight_decay=weight_decay,\n    logging_steps=50,\n    save_steps=500,\n    save_total_limit=2,\n    eval_strategy=\"no\",\n    report_to=\"wandb\",  # Log training metrics to Weights and Biases\n    fp16=torch.cuda.is_available(),\n    dataloader_num_workers=2,\n    optim=optimizer,\n    lr_scheduler_type=lr_scheduler_type,\n    seed=random_state\n)","metadata":{"_uuid":"2410d343-df7a-4176-b04a-40ad021d3b2c","_cell_guid":"bd8d3123-4b08-4a1a-be0c-043a69c0bcb1","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-07T04:26:14.270365Z","iopub.execute_input":"2024-10-07T04:26:14.271295Z","iopub.status.idle":"2024-10-07T04:26:14.307624Z","shell.execute_reply.started":"2024-10-07T04:26:14.271252Z","shell.execute_reply":"2024-10-07T04:26:14.306680Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"## Initialize Trainer","metadata":{"_uuid":"1b9b7fb9-f530-4f56-8716-d33e2517ceb6","_cell_guid":"2fe311f7-2862-4211-91fd-ecf687df965b","trusted":true}},{"cell_type":"code","source":"from transformers import DataCollatorWithPadding\n\ndata_collator = DataCollatorWithPadding(tokenizer, padding=True)","metadata":{"_uuid":"14e3da49-8f88-4d0e-b3b4-9048a234b0f0","_cell_guid":"aaec73e1-6f47-4fbb-bce0-6969fca02b10","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-07T04:26:16.869715Z","iopub.execute_input":"2024-10-07T04:26:16.870584Z","iopub.status.idle":"2024-10-07T04:26:16.875880Z","shell.execute_reply.started":"2024-10-07T04:26:16.870542Z","shell.execute_reply":"2024-10-07T04:26:16.874853Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n)","metadata":{"_uuid":"afda97ff-86ce-4ad2-9094-78db11301ffb","_cell_guid":"80f64d28-fe21-41fc-9c90-edd193c7a8db","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-07T04:26:21.489625Z","iopub.execute_input":"2024-10-07T04:26:21.490049Z","iopub.status.idle":"2024-10-07T04:26:21.508549Z","shell.execute_reply.started":"2024-10-07T04:26:21.490010Z","shell.execute_reply":"2024-10-07T04:26:21.507656Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\nmax_steps is given, it will override any value given in num_train_epochs\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Initialize Weights & Biases for Tracking (Optional)","metadata":{"_uuid":"25e6b830-3a7e-472f-966c-7bfb3135a9d2","_cell_guid":"68357d7a-3a67-4ce5-bd51-f36a6f85ed4d","trusted":true}},{"cell_type":"code","source":"wandb.init(project=\"biobart-finetuning\", name=\"BioBart-Fine-Tuning\")","metadata":{"_uuid":"fccf4cdf-af4b-4817-b167-0202f219e88f","_cell_guid":"65e498f7-19ca-4e73-8bae-bac6c6f83eb7","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-07T04:26:26.185367Z","iopub.execute_input":"2024-10-07T04:26:26.186238Z","iopub.status.idle":"2024-10-07T04:26:28.810627Z","shell.execute_reply.started":"2024-10-07T04:26:26.186197Z","shell.execute_reply":"2024-10-07T04:26:28.809669Z"},"trusted":true},"execution_count":34,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Finishing last run (ID:6oo3esu6) before initializing another..."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.021 MB of 0.021 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f92ea2308d1b4494b1a7df726703e41c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">BioBart-Fine-Tuning</strong> at: <a href='https://wandb.ai/jpsrinivasan38-roxs/biobart-finetuning/runs/6oo3esu6' target=\"_blank\">https://wandb.ai/jpsrinivasan38-roxs/biobart-finetuning/runs/6oo3esu6</a><br/> View project at: <a href='https://wandb.ai/jpsrinivasan38-roxs/biobart-finetuning' target=\"_blank\">https://wandb.ai/jpsrinivasan38-roxs/biobart-finetuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20241007_041501-6oo3esu6/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Successfully finished last run (ID:6oo3esu6). Initializing new run:<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241007_042626-04zprf3t</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/jpsrinivasan38-roxs/biobart-finetuning/runs/04zprf3t' target=\"_blank\">BioBart-Fine-Tuning</a></strong> to <a href='https://wandb.ai/jpsrinivasan38-roxs/biobart-finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/jpsrinivasan38-roxs/biobart-finetuning' target=\"_blank\">https://wandb.ai/jpsrinivasan38-roxs/biobart-finetuning</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/jpsrinivasan38-roxs/biobart-finetuning/runs/04zprf3t' target=\"_blank\">https://wandb.ai/jpsrinivasan38-roxs/biobart-finetuning/runs/04zprf3t</a>"},"metadata":{}},{"execution_count":34,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/jpsrinivasan38-roxs/biobart-finetuning/runs/04zprf3t?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x77fb50332b30>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Start Training","metadata":{"_uuid":"bc27cdee-4659-49fe-b01a-ca1af18af5be","_cell_guid":"165a11d0-4554-4dc6-9959-ffa10866920f","trusted":true}},{"cell_type":"code","source":"trainer.train()","metadata":{"_uuid":"e941e8f3-7971-439e-9af2-8e61cfac073d","_cell_guid":"86c57b6b-e781-4d02-b21c-45414ce7e40a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-07T04:26:31.505422Z","iopub.execute_input":"2024-10-07T04:26:31.505813Z","iopub.status.idle":"2024-10-07T04:48:31.635436Z","shell.execute_reply.started":"2024-10-07T04:26:31.505777Z","shell.execute_reply":"2024-10-07T04:48:31.634447Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1000/1000 21:55, Epoch 4/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>0.175000</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.008000</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.006500</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.005600</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.003800</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.004100</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.003500</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.003600</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.002200</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.002100</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.002200</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.002100</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.001400</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.001400</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.001400</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.000900</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.000900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n","output_type":"stream"},{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1000, training_loss=0.011381900995969772, metrics={'train_runtime': 1319.0126, 'train_samples_per_second': 24.261, 'train_steps_per_second': 0.758, 'total_flos': 2438031228272640.0, 'train_loss': 0.011381900995969772, 'epoch': 4.662004662004662})"},"metadata":{}}]},{"cell_type":"code","source":"!rm -rf /kaggle/working/*","metadata":{"execution":{"iopub.status.busy":"2024-10-07T05:01:37.416059Z","iopub.execute_input":"2024-10-07T05:01:37.416466Z","iopub.status.idle":"2024-10-07T05:01:40.129207Z","shell.execute_reply.started":"2024-10-07T05:01:37.416426Z","shell.execute_reply":"2024-10-07T05:01:40.128148Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Evaluation","metadata":{"_uuid":"4c8ee02d-ea3e-424e-b547-7dc96cbf0255","_cell_guid":"769ace9a-9e66-4fba-a48a-e6618fffb131","trusted":true}},{"cell_type":"code","source":"# Use HuggingFace's dataset splitting method instead of train_test_split\ntrain_test = tokenized_dataset.train_test_split(test_size=0.1)\n\n# Access the train and validation datasets\ntrain_dataset = train_test['train']\nval_dataset = train_test['test']\n\n# Define the trainer again with the validation dataset\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n)\n\n# Start evaluation\neval_results = trainer.evaluate()\nprint(f\"Evaluation Results: {eval_results}\")\n","metadata":{"_uuid":"dee79f12-e55e-46f2-81ba-39ae026790a8","_cell_guid":"f3893ab2-08ff-4854-9471-bd340946138c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-07T04:50:22.836702Z","iopub.execute_input":"2024-10-07T04:50:22.837842Z","iopub.status.idle":"2024-10-07T04:50:31.919197Z","shell.execute_reply.started":"2024-10-07T04:50:22.837795Z","shell.execute_reply":"2024-10-07T04:50:31.918215Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stderr","text":"max_steps is given, it will override any value given in num_train_epochs\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='43' max='43' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [43/43 00:08]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Evaluation Results: {'eval_loss': 0.0005446386057883501, 'eval_runtime': 9.0066, 'eval_samples_per_second': 76.277, 'eval_steps_per_second': 4.774}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Testing Model Responses","metadata":{"_uuid":"5af176e6-4c39-4aef-a27a-38fb08b41be0","_cell_guid":"1dd2d54c-2a25-4379-9546-efb08e28fc1e","trusted":true}},{"cell_type":"code","source":"def test_model_response(inputs):\n    model.eval()  # Set the model to evaluation mode\n    with torch.no_grad():\n        device = next(model.parameters()).device\n        # Tokenize the inputs and move to the correct device\n        inputs = tokenizer(inputs, return_tensors=\"pt\", padding=True, truncation=True)\n        inputs = {key: value.to(device) for key, value in inputs.items()}\n        \n        # Use sampling with specified temperature and top_p\n        outputs = model.generate(\n            inputs['input_ids'], \n            max_length=150, \n            do_sample=True,   # Enable sampling\n            temperature=0.9, \n            top_p=0.95,\n            num_return_sequences=1  # Number of sequences to return for each input\n        )\n        \n        # Decode outputs for each input\n        generated_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n        \n        return [text.strip() for text in generated_texts]  # Return a list of generated responses\n\n# Sample inputs to test\ntest_inputs = [\n    \"What should I do if I have a headache?\",\n    \"I'm feeling very tired and unwell\",\n    \"What are the symptoms of fever?\"\n]\n\n# Print responses for each input\nresponses = test_model_response(test_inputs)\nfor input_text, response in zip(test_inputs, responses):\n    print(f\"Input: {input_text}\\nResponse: {response}\\n\")\n","metadata":{"_uuid":"3001f17c-fcd0-4135-8ade-075cc98ae20a","_cell_guid":"e413c98e-0ef3-4f7b-a31f-cc6abd74010d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-07T04:52:13.932145Z","iopub.execute_input":"2024-10-07T04:52:13.933042Z","iopub.status.idle":"2024-10-07T04:52:14.094455Z","shell.execute_reply.started":"2024-10-07T04:52:13.932999Z","shell.execute_reply":"2024-10-07T04:52:14.093508Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"Input: What should I do if I have a headache?\nResponse: Headache\n\nInput: I'm feeling very tired and unwell\nResponse: Tired and unwell\n\nInput: What are the symptoms of fever?\nResponse: Fever\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Save Fine-Tuned Model","metadata":{"_uuid":"17b9fcaf-7972-4250-9de5-08aa82899947","_cell_guid":"1fb44d48-de32-44f2-9a2a-48db52eedb6e","trusted":true}},{"cell_type":"code","source":"model.save_pretrained(output_dir)\ntokenizer.save_pretrained(output_dir)","metadata":{"_uuid":"8e8b5002-b56e-4e50-9fc7-0b986661d3e6","_cell_guid":"b7b73312-92da-40c5-877d-d4fa4994c585","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":49,"outputs":[{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n","output_type":"stream"},{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/saved_model/tokenizer_config.json',\n '/kaggle/working/saved_model/special_tokens_map.json',\n '/kaggle/working/saved_model/vocab.json',\n '/kaggle/working/saved_model/merges.txt',\n '/kaggle/working/saved_model/added_tokens.json',\n '/kaggle/working/saved_model/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"import shutil\nimport os\n\nshutil.make_archive(output_dir, 'zip', output_dir)\n\nzip_file_path = f\"{output_dir}.zip\"\ndestination_path = \"/kaggle/working/bertV2.zip\"\n\n# Check if the destination zip file already exists, and if so, remove it\nif os.path.exists(destination_path):\n    os.remove(destination_path)\n\nshutil.move(zip_file_path, destination_path)\n\n# Print the location of the downloaded zip file\nprint(f\"Zip file created and moved to: {destination_path}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-07T05:05:05.592496Z","iopub.execute_input":"2024-10-07T05:05:05.592926Z","iopub.status.idle":"2024-10-07T05:05:44.021080Z","shell.execute_reply.started":"2024-10-07T05:05:05.592885Z","shell.execute_reply":"2024-10-07T05:05:44.020075Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"Zip file created and moved to: /kaggle/working/bertV2.zip\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"/kaggle/working/bertV2.zip\")","metadata":{"execution":{"iopub.status.busy":"2024-10-07T05:09:42.606659Z","iopub.execute_input":"2024-10-07T05:09:42.607630Z","iopub.status.idle":"2024-10-07T05:09:42.613391Z","shell.execute_reply.started":"2024-10-07T05:09:42.607578Z","shell.execute_reply":"2024-10-07T05:09:42.612428Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"/kaggle/working/bertV2.zip\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}